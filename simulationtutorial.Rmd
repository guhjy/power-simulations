---
title: "Intro to Power Analysis Using Simulation Methods"
author: "Jessie Sun"
date: "15/11/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a very basic introduction to conducting power analyses based on simulations, using the *lavaan* package.

## Types of Power Analysis

1. *A priori power analysis:* What is the smallest sample size we need to have 80% power to detect an effect size of interest (e.g. $\beta$ = 0.20, $\beta$ = 0.50), at an alpha level of .05?
2. *Sensitivity power analysis:* What is the smallest effect size we can detect with 80% power, given our sample size, at an alpha level of .05?
3. *Post-hoc power analysis:* What power did we have to detect the observed effect size, given the sample size actually used, at an alpha level of .05?

As Daniel Lakens has [explained elsewhere](http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html), observed power (from a post-hoc power analysis) is a useless statistical concept. Thus, this tutorial will focus on a priori and sensitivity power analyses.

(Note: You can also set different alpha thresholds and power goals, but to keep things simple, let's assume the standard .05 alpha threshold and the goal of 80% power.)

## The Model

As shown in Figure 1 below, we will be considering a fairly simple model, with two predictor variables $X_1$ and $X_2$, and one outcome variable, Y. All variables are observed.

```{r, out.width = "300px", echo = FALSE}
knitr::include_graphics("model.png")
```

*Figure 1.* Labelled multiple regression model. 

## Our Goal 

Our goal in this tutorial is to conduct a priori and sensitivity power analyses for the regression path for Y on $X_2$ ($\beta_{2}$). \pagebreak

## Model Assumptions

This model includes the following parameters:

-Variance of $X_1$ ($s^2_{X1}$)

-Variance of $X_2$ ($s^2_{X2}$)

-Covariance between X1 and X2 ($cov_{X1,X2}$)

-Regression path for Y on X1 ($\beta_{1}$)

-Regression path for Y on X2 ($\beta_{2}$)

-Residual variance of Y ($\epsilon_{Y}$)

In this case, to conduct power analysis based on standardized effect sizes, we will: 

1. Fix the variance of $X_1$ and $X_2$ to 1.
2. Assume that Y has a variance of 1, such that the residual variance $\epsilon{Y}$ (i.e., the variance not explained by $X_1$ and $X_2$) will be equal to 1 - ($\beta_{1}$^2^ + $\beta_{2}$^2^). 

Note that since we are using a standardized metric, $cov_{X1,X2}$ is now simply the correlation between $X_1$ and $X_2$ (i.e., $r_{X1,X2}$).

To simulate data based on a **population model**, we need to make assumptions about each of the other parameters. Some of these assumptions might be based on existing data (e.g., you might already know how strongly $X_1$ and $X_2$ tend to be correlated in the literature), but at other times, they might seem somewhat arbitrary. Because of the potential arbitrariness of our assumptions, it is useful to simulate power under a range of different assumptions (e.g., what if the correlation between $X_1$ and $X_2$ was stronger, or if the regression path for $\beta_{1}$ was larger or smaller?).

However, for the purposes of this tutorial, let's assume that $X_1$ and $X_2$ are moderately positively correlated (*r* = .30), and that $X_1$ positively predicts Y to a small extent ($\beta_{1}$ = 0.10).

Figure 2 illustrates these assumptions.

```{r, out.width = "300px", echo = FALSE}
knitr::include_graphics("assumptions.png")
```

*Figure 2.* Labelled multiple regression model with model assumptions.

## A Priori Power Analysis

Let's get started with an a priori power analysis. What is the smallest sample size we need to have 80% power to detect an effect size of $\beta_2$ = 0.20, at an alpha level of .05?

First, we need to load the *lavaan* package

```{r}
library(lavaan)
```

Next, we need to specify the population model, based on the assumptions in Figure 2, plus our effect size of interest ($\beta_2$ = 0.20). This is the model that, at the population level, we assume is generating the data that we might see in any given dataset.

Basic *lavaan* notation: a double ~~ denotes variances and covariances, whereas a single ~ denotes a regression path.

```{r}

popmod1 <- '
# variances are fixed at 1
x1~~1*x1
x2~~1*x2
x3~~1*x3
x4~~1*x4
x5~~1*x5
x6~~1*x6
x7~~1*x7
x8~~1*x8
xx1~~1*xx1

# correlation between X1 and X2 is assumed to be .30
# x1~~.3*x2

# regression path is assumed to be .10
y~.10*x1
y~.10*x2
y~.10*x3
y~.10*x4
y~.10*x5
y~.10*x6
y~.10*x7
y~.10*x8

x1~.10*xx1
x2~.10*xx1
x3~.10*xx1
x4~.10*xx1
x5~.10*xx1
x6~.10*xx1
x7~.10*xx1
x8~.10*xx1

# # residual variance of Y is 1 - (.1^2 + .2^2) = .95
# y~~.95*y
'

```

We also need to create another *lavaan* model, without those population-level assumptions.

```{r}

fitmod <- '
# # variances of X1 and X2
# x1~~x1
# x2~~x2
# x3~~x3
# x4~~x4
# x5~~x5
# x6~~x6
# x7~~x7
# x8~~x8

# # correlation between X1 and X2
# x1~~x2

# regression path for Y on X1
y~x1
y~x2
y~x3
y~x4
y~x5
y~x6
y~x7
y~x8

# regression path of interest, Y on X2
x1~xx1
x2~xx1
x3~xx1
x4~xx1
x5~xx1
x6~xx1
x7~xx1
x8~xx1


# # residual variance of Y
# y~~y
'
```

To see the logic of the simulation process, let's first just simulate one dataset based on the population model, popmod1.

```{r}
set.seed(20181102)  # setting a seed for reproducibility of the example
data <- simulateData(popmod1, sample.nobs = 500)  # assume 500 participants for now
```

Now, we're going to fit our model (fitmod) to this dataset.

```{r}
fit <- sem(model = fitmod, data=data, fixed.x=F)
```

Here are the parameter estimates. The parameter of interest, y  ~  x2, is in row 5. As you can see, this parameter was statistically significant (*p* < .005) in this simulation based on one dataset.

```{r}
parameterEstimates(fit)  # see all parameters
parameterEstimates(fit)[5,]  # isolating the row with the parameter of interest
```

However, to estimate power, we need to simulate many datasets. Then, we can obtain the % of datasets in which the parameter of interest is statistically significant. This is our power estimate.

So, let's go ahead and simulate 1000 datasets, still assuming a sample size of 500.

```{r}
results <- NULL  # create empty object to store results

for (i in 1:10){  # simulating 1000 datasets
  data <- simulateData(popmod1, sample.nobs = 500)  # each dataset contains 500 participants
  fit <- sem(model = fitmod, data=data, fixed.x=F)  # fit the model to each dataset
  results <- rbind(results,parameterEstimates(fit)[5,])  # save the row for y ~ x2 for each dataset
}

# Count the proportion of significant parameter estimates out of 1000 datasets
paste0('Estimated power = ',mean(results$pvalue < .05))
```

As you can see, power in this example was excellent; we were able to detect the effect of interest in 99.4% of those 1,000 simulations.

Now, let's simulate some results based on different sample sizes. It's a good idea to start with rough increments (e.g., *N* = 100, 200, 300, 400) and fewer datasets (e.g., 100) to save computing time. Then once you know which ballpark to aim for, re-run the simulations based on narrower increments (e.g., increments of *N* = 10) and more datasets (e.g., 1000) to get a more exact estimate of sample requirements.

```{r}
# now that we are trying a few different sample sizes, we need to create a list to store these results
powerlist <- list()  

# extending the for() loop with a loop for different sample sizes
for(j in seq(100,400,by=100)){  # trying sample sizes of 100, 200, 300, 400
  results <- NULL 
  for (i in 1:100){  # starting with 100 datasets for each sample size
    data <- simulateData(popmod1, sample.nobs = j)  # j corresponds to the sample size
    fit <- sem(model = fitmod, data=data, fixed.x=F)
    results <- rbind(results,parameterEstimates(fit)[5,])  # row for y ~ x2
    powerlist[[j]] <- mean(results$pvalue < .05)
  }
}

# Convert the power list into a table
library(plyr)
powertable <- ldply(powerlist)
names(powertable)[1] <- c('power')

# Add a column for the sample size
powertable$N <- seq(100,400,by=100)

# Here are all the power estimates:
powertable

# Conclusion:
paste0('The smallest sample size that provided at least 80% power was N = ',
       powertable[which(powertable$power>.80),'N'][1])

```

Based on 200 simulations, it seems like the ballpark for 80% power is between 150 and 250 participants. So now let's re-run the simulations, but in increments of *N* = 10, and with 1000 simulations each.

```{r}
# create a list to store these results for different sample sizes
powerlist <- list()  

# extending the for() loop with a loop for different sample sizes
for(j in seq(150,250,by=10)){  # trying sample sizes between 150 to 250, in increments of 10
  results <- NULL 
  for (i in 1:1000){  # now simulating 1000 datasets for each sample size
    data <- simulateData(popmod1, sample.nobs = j)  # j corresponds to the sample size
    fit <- sem(model = fitmod, data=data, fixed.x=F)
    results <- rbind(results,parameterEstimates(fit)[5,])  # row for y ~ x2
    powerlist[[j]] <- mean(results$pvalue < .05)
  }
}

# Convert the power list into a table
powertable <- ldply(powerlist)
names(powertable)[1] <- c('power')

# Add a column for the sample size
powertable$N <- seq(150,250,by=10)

# Here are all the power estimates:
powertable

# Conclusion:
paste0('The smallest sample size that provided at least 80% power was N = ',
       powertable[which(powertable$power>.80),'N'][1])

```
